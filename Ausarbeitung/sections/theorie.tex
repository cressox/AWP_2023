Zur Müdigkeitserkennung im Zusammenhang mit dem Autofahren gibt es verschiedene Methoden, wie sie von Arunasalam et al. \cite{AR20} und Ramzan et al. \cite{RA19} beschrieben werden. Diese lassen sich in drei Hauptkategorien unterteilen:

\begin{itemize}
\item Physiologische Methoden: Hierzu gehören Messungen wie Puls und EEG.
\item Fahrverhaltensbasierte Methoden: Dies umfasst die Analyse des Fahrverhaltens mittles Sensoren am Auto.
\item Verhaltensbasierte Methoden: Hierbei liegt der Fokus auf Beobachtungen von Auge, Gesicht und Kopfbewegungen.
\end{itemize}

Die visuelle Analyse des Verhaltens bietet eine praktikable Lösung, da diese auch vielfältig einsetzbar ist, bepsilesweise auch um Müdugkeit beim Lernen und Arbeiten zu erkennen.

Unser Hauptziel, wie bereits in der Aufgabenstellung erwähnt, besteht darin, eine frühzeitige Warnung auszugeben, wenn eine Person Anzeichen von Müdigkeit zeigt. Wir möchten damit eine Methode entwickeln, um Sekundenschlaf frühzeitig zu verhindern. Dies erfordert die Erkennung von Müdigkeit sowie die Identifizierung von Ablenkung (z. B. wenn eine Person zu lange nicht nach vorne schaut). Die Herausforderung besteht darin, den Zeitpunkt zu bestimmen, ab dem eine Person als müde gelten kann, und objektive Kriterien dafür zu entwickeln. Eine weitere wichtige Herausforderung besteht darin, dass Müdigkeit bei jeder Person unterschiedlich ist und sich visuelle Verhaltensweisen individuell manifestieren, beispielsweise variiert die Blinzelrate erheblich von Person zu Person, selbst bei Ruhe (Bentivoglio et al. \cite{BE97} berichtet von 4 bis zu 48 Blinzelschlägen pro Minute).

Unsere Vorgehensweise orientiert sich an Ghodoosian et al. \cite{GH19} und verwendet deren Datensatz als Ausgangspunkt. Wir verarbeiten Videodaten einer Person und extrahieren Landmarks, um quantitative Merkmale zu berechnen. Unsere Schwerpunkte liegen auf den visuellen Eigenschaften des Auges, wobei die Erkennung von Blinzeln eine entscheidende Rolle spielt. Um die Ergebnisse auf individuelle Unterschiede abzustimmen, führen wir eine Kalibrierung durch, die absolute Daten in relative Werte umwandelt. Diese Merkmale werden dann für die Klassifikation von \glqq müde\grqq{} und \glqq nicht müde\grqq{} verwendet, woraufhin bei \glqq müde\grqq{} immer wieder eine Warnung ertönt. Eine akustische Warnung ertönt ebenfalls, wenn eine Person in Sekundenschlaf verfällt und die Augen über einen längeren Zeitraum geschlossen sind.

\subsection{Gesichtserkennung}
\label{sec:facedetection}

\begin{figure}
    \centering
    \begin{subfigure}{0.3\textwidth}
        \includegraphics[width=\linewidth]{images/MPFaceMesh1.png}
        \caption{Die Landmarks des Mediapipe Face Mesh mit Nummerierungen.}
        \label{fig:MPFaceMesh1}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.3\textwidth}
        \includegraphics[width=\linewidth]{images/MPFaceMesh2.png}
        \caption{Alle Landmarks des Face Mesh während der Anwendung.}
        \label{fig:MPFaceMesh2}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.3\textwidth}
        \includegraphics[width=\linewidth]{images/MPFaceMesh3.png}
        \caption{Die Augenlandmarks des Face Mesh zur Berechnung der EAR.}
        \label{fig:MPFaceMesh3}
    \end{subfigure}
    \caption{Prozess der Extraktion der entscheidenden Landmarks für unsere Müdigkeitsdetektion.}
    \label{fig:MPFaceMesh}
\end{figure}

Um verwertbare Daten zu generieren, ist die Erkennung der Augenstruktur in Echtzeit entscheidend, um Blinezlschläge zu identifizieren, was die Grundlage für weitere Analysen bildet. Eine höhere Bildwiederholrate (auch Framerate; Bilder pro Sekunde) ermöglicht präzisere Ergebnisse, obwohl die meisten Kameras und Videodaten üblicherweise eine Framerate von etwa 30 Bildern pro Sekunde aufweisen. Es ist von Bedeutung, dass die Framerate durch die Berechnungen des Algorithmus nicht erheblich verlangsamt wird. 

Ein leistungsstarkes und schnelles System dafür ist Mediapipe \cite{LU19}. Das Mediapipe Face Mesh erkennt in Echtzeit 468 3D-Gesichtslandmarks, selbst auf mobilen Geräten, und ermöglicht die Approximation einer 3D-Oberfläche des Gesichts durch maschinelles Lernen. Für unsere Anwendung verwenden wir Version 0.9.0, um eine Integration mit der Kivy-Bibliothek zu ermöglichen, da eine neuere Version von Mediapipe auf unerklärliche Probleme stößt. In der Version 0.9.0 des Mediapipe Face Mesh bleibt die Grundeinstellung statisch und bewegt sich als Ganzes mit, was jedoch für unsere Untersuchung unzureichend ist. Daher muss die Einstellung \glqq refine\grqq{} auf \glqq true\grqq{} gesetzt werden, um zehn zusätzliche Landmarks und Bewegungen innerhalb des Gesichts zu erfassen. Darüber hinaus muss der \glqq static image mode\grqq{} auf \glqq false\grqq{} gesetzt werden, um das Face Mesh auf Videos anzuwenden. 

Wir haben auch andere Ansätze für ein Face Mesh evaluiert, darunter den Dlib Face Landmark Detector (Davis E. King \cite{DLIB09}), basierend auf Dalal et al. \cite{DA05}. Im Vergleich zum Mediapipe Face Mesh erzielte dieser jedoch in Bezug auf Genauigkeit schlechtere Ergebnisse und zeigte gelegentlich Aussetzer. Weitere Details dazu befinden sich in der Evaluation (siehe Kapitel \ref{subsec:selftest}).

\subsection{Blinzeldetektion}
\label{sec:blinkdetection}

\begin{figure}
    \centering
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\linewidth]{images/EyeLandmarks.jpg}
        \caption{Die sechs Landmarks von P1 bis zu P6 in ihrer Anordnung am Auge \cite{DE22}}
        \label{fig:eyelandmarks}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.45\textwidth}
        \includegraphics[width=\linewidth]{images/EARCurve.png}
        \caption{Vereinfachte Darstellung eines Blinzelvorgangs und der Auswirkung auf die EAR.}
        \label{fig:earcurve}
    \end{subfigure}
    \caption{Prozess der Berechnung eines Blinzlers über die sechs Augenlandmarks (a) über die EAR Kurve und dem Blink Threshold (b)}
    \label{fig:eyeaspectratio}
\end{figure}

Durch die Berechnung von Landmarks können wir nun das Blinzeln einer Person erkennen. Eine verbreitete Methode hierfür ist die Berechnung der 	\glqq Eye Aspect Ratio\grqq{} (EAR), wie von Soukupova \cite{SO16} präsentiert. Die EAR wird anhand des Auges und sechs Landmarks (P1 bis P6) ermittelt und entspricht dem Verhältnis der vertikalen Öffnung zur horizontalen Öffnung:

\begin{equation}
	\text{EAR} = \frac{{\|P2 - P6\| + \|P3 - P5\|}}{{2\|P1 - P4\|}}
\end{equation}

In dieser Formel repräsentieren P1 bis P6 die Koordinaten der Landmarks, wobei P1 und P4 die äußeren Augenwinkel sind, P2 und P6 die oberen Augenlider und P3 und P5 die unteren Augenlider (Abbildung \ref{fig:eyelandmarks}). Die $ \| \| $ Symbole stellen die euklidische Norm dar, die den Abstand zwischen den Punkten berechnet.

Die EAR ist stets größer, wenn das Auge geöffnet ist, im Vergleich dazu, wenn es geschlossen ist. Allerdings variieren diese Werte zwischen Personen, wie auch unser eigener Test zeigt, beispielsweise beträgt der durchschnittliche Wert für Proband 1 circa 0.295, wohingegen Proband 2 einen Wert von 0.252 aufweist (siehe Kapitel \ref{subsec:selftest}). Der Durchschnitts-EAR wird über beide Augen berechnet und sichert die kontinuierliche Erfassung dieses Werts für jedes Einzelbild, was weitere Analysen ermöglicht:

\begin{equation}
	\text{Durchschnittliche EAR} = \frac{{\text{EAR}_{\text{linkes Auge}} + \text{EAR}_{\text{rechtes Auge}}}}{2}
\end{equation}

Bei einem Blinzelschlag zeigt die EAR-Kurve einen typischen Verlauf, der in Abbildung \ref{fig:earcurve} dargestellt ist - sie sinkt zunächst parabelförmig ab und steigt dann wieder an. Ein wichtiger Aspekt ist nun der Schwellenwert, um zu erkennen, ab wann das Auge als geschlossen oder offen gilt, um ein Blinzeln zu detektieren und seine Dauer zu berechnen. In unseren Selbsttests (siehe Kapitel \ref{subsec:selftest}) hat sich vorläufig ein fester Schwellenwert von 0.16 als am besten geeignet erwiesen. Sobald die EAR den Schwellenwert unterschreitet, wird das Auge als geschlossen betrachtet, und wenn er wieder überschritten wird, wird der Vorgang als ein vollständiges Blinzeln erkannt. Die Blinzelzeit kann anhand der Anzahl der Frames ermittelt werden, in denen der EAR-Wert kleiner als 0.16 ist. Um verschiedene Bildwiederholungsraten auszugleichen, wird die Framerate des Videos verwendet, um einen zeitlichen Wert in Millisekunden zu erhalten. Dies ist entscheidend für die Trainingsphase des Klassifikators und die Anwendbarkeit über verschiedene Bildwiederholungsraten und Geräte hinweg.  Allerdings sollten weitere Untersuchungen zur Verbesserung der Genauigkeit sowie zur Berücksichtigung individueller Unterschiede und möglicher Augenkrankheiten wie Grauer Star oder altersbedingter Verengung der Augen in zukünftigen Arbeiten durchgeführt werden. Eine agile Berechnung der EAR-Schwelle zu Beginn jeder Aufnahme für jede Person könnte eine Möglichkeit sein, wie es von Ghoddoosian et al. \cite{GH19} in ihrem \glqq Blink Retrieval Algorithm\grqq{} beschrieben wird.

\subsection{Features}
\label{sec:features}

Der nächste Schritt in unserem Verarbeitungsprozess besteht darin, Features zu sammeln. Insbesondere für das Auge stehen verschiedene mögliche Features zur Verfügung und unterschiedliche Kombinationen können zu besseren Ergebnissen führen, wie von Dreißig et al. \cite{DREI} untersucht wurde. Dreißig et al. hat zusätzlich Kopfbewegungen in seine Untersuchungen einbezogen. Auch Ebrahim \cite{EB16} stellt 19 Features vor, die sich ausschließlich auf den Blinzelvorgang beziehen.

Für uns haben sich folgende Features bewährt: die einfache Betrachtung der \glqq Eye Aspect Ratio\grqq{} (EAR), die durchschnittliche Blinzeldauer und der \glqq Percentage of Eye Closure\grqq{}-Wert (PERCLOS). Die EAR bildet die Grundlage für weitere Features und besitzt auch allein schon eine gewisse Aussagekraft. Wir betrachten die gemittelte EAR über eine Zeitspanne und vergleichen sie mit vorherigen Zeitspannen. Außerdem analysieren wir die gemittelte EAR über alle Werte kleiner als 0.16, um die Werte zu entfernen, wenn das Auge als geschlossen gilt.

Ein weiteres Feature ist die durchschnittliche Blinzeldauer. Wenn eine Person müde wird, sollte die Blinzeldauer länger sein. Daher speichern wir für jeden Blinzelvorgang die Dauer in Millisekunden und berechnen den Durchschnitt über eine Zeitspanne.
Der PERCLOS-Wert, erstmals von Wierwille et al. \cite{WI94} beschrieben, repräsentiert den Anteil der Zeit in einer Minute, in der die Augen zu mindestens 80 Prozent geschlossen sind. Bei müden Personen wird dieser Wert tendenziell größer. Für eine einfachere Berechnung verwenden wir den PERCLOS-Wert als den Anteil der Zeit in einer Minute, in der der EAR-Wert unterhalb des Schwellenwerts von 0.16 liegt, was darauf hinweist, dass das Auge als geschlossen betrachtet wird.

Die Frage nach der Festlegung der Zeitspanne für diese Berechnungen stellt sich nun als nächstes. Unsere Recherchen ergaben, dass eine Dauer von 60 Sekunden als angemessene Spanne angesehen wird, da Werte wie PERCLOS pro Minute berechnet werden. Diese Berechnungen erfolgen überlappend, was bedeutet, dass bei jedem neuen Frame neue Werte für die Features generiert werden.

Ein Beispielhafter Feature-Vektor nach der Berechnung ist in Tabelle \ref{table:featurevector} zu erkennen.

\begin{table}[!ht]
    \centering
    \begin{tabular}{|l|l|l|l|l|}
    \hline
        ~ & PERCLOS (\%) & Blinzeldauer (ms) & EAR bei geöffnete Augen & EAR \\ \hline
        Wach & 0.0094 & 80.24 & 0.2639 & 0.2627 \\ \hline
        Fraglich & 0.0879 & 174.77 & 0.2267 & 0.2188 \\ \hline
        Müde & 0.1780 & 168.60 & 0.1985 & 0.1878 \\ \hline
    \end{tabular}
\caption{Beispielhafter Featurevektor}
\label{table:featurevector}
\end{table}

\subsection{Kalibrierung}
\label{sec:calibration}

Eine spezifische Herausforderung, die wir in Anbetracht unserer Aufgabenstellung betrachten müssen, ist die Individualität in Bezug auf die Augenöffnung und das Verhalten bei Müdigkeit. Bei verschiedenen Personen zeigt sich Müdigkeit auf unterschiedliche Weisen, bei einer Person kann die Blinzeldauer länger werden, während bei einer anderen Person eher die Blinzelhäufigkeit zunimmt. Zusätzlich variiert die EAR bei jeder Person erheblich, wie auch unser Selbsttest (siehe Kapitel \ref{subsec:selftest}) bestätigt hat. Daher ist eine Relativierung der Werte unerlässlich.

Zu Beginn jeder Aufzeichnung gehen wir davon aus, dass die Person wach ist und verwenden die ersten 60 Sekunden, um Referenzwerte für den Wachzustand zu erfassen. Diese Zeitspanne entspricht derjenigen, die für die weiteren Berechnungen verwendet wird. Die für jeden neuen Frame berechneten Werte werden in Beziehung zu den Werten des Wachzustands gesetzt. Auf diese Weise erhalten wir immer relative Werte für jedes einzelne Feature.


\subsection{Datensatz}
Bei dem ausgewählten Datensatz zum Trainieren des Klassifikators handelt es sich um den UTA Real-Life Drowsiness Dataset von Ghodoosian et al. \cite{GH19}. Der Datensatz stand unglücklicherweise nicht vollständig zur Verfügung, da auf der offiziellen Seite der Veröffentlichenden technische Probleme zur Zeit der Ausarbeitung vorlagen. In dem verwendeten Datensatz befinden sich jeweils drei Videos von 48 Individuen. Die teilnehmenden Personen wurden so ausgewählt, dass verschiedene Ethnien, Altersgruppen und Geschlechter abgedeckt sind. Weiterhin trug ein Teil der Probanden eine Brille. Die Aufnahmen entstanden jeweils in natürlicher Umgebung mit verschiedenen Perspektiven, Hintergründen und Belichtungsverhältnissen. Pro Person ist ein Video für die Klasse „wach“, „fraglich“ und „müde“ gegeben. 

Die drei Kategorien ergeben sich, wie in Abbildung \ref{fig:KSSScale} erkenntlich, durch eine Zusammenfassung des offiziellen Karolinska Sleepiness Scale (KSS). Die KSS wurde von Akerstedt et al. \cite{AK90} zuerst vorgestellt. Als „wach“ werden die Stufen eins bis drei betitelt, wobei für „fraglich“ die Stufen sechs und sieben repräsentativ sind. Auf den Zustand „müde“ weisen die Einordnungen in die Stufen acht und neun hin. Die durchschnittliche Länge eines einzelnen Videos beträgt in etwa zehn Minuten, so dass die für die Klassifikation benötigten Featurewerte über einen längeren Zeitraum möglichst genau und fehlerunabhängig ermittelt werden können. 

\begin{figure}
\centering
\includegraphics[scale=0.6]{images/KSSScale.png}
\caption{Karolinska Sleepiness Scale (KSS)}
\label{fig:KSSScale}
\end{figure}

Zu beachten bei der vorliegenden Datengrundlage ist die Tatsache, dass die Videos über verschiedene Dateiformate, Frameraten und Längen verfügen. Einerseits sichert dies die Robustheit des Programms, um mit verschiedener Hardware umgehen zu können, jedoch war andererseits für das automatisierte Einlesen und Featurextrahieren ein Mehraufwand benötigt. 

Es wurde ein Skript erstellt, welches alle Videos mit dem entsprechenden Datenformat aufruft und abspielt und dabei die Extraktion der Features über eine bestimmte Zeit laufen lässt, sodass aus jedem Video relevante Informationen gewonnen werden, alle Werte aus den Videos gleich stark gewichtet sind und zusätzlich kein Video vor einer vollständigen Extraktion aufgrund zu kurzer Aufnahmedauer eines Probanden abbricht.


\subsection{Klassifikation}
\label{sec:classification}

In Bezug auf die drei Videos pro Person setzen wir für jedes Feature den Zustand \glqq fraglich\grqq{} und \glqq müde\grqq{} in Relation zum Wachzustand und führen daraufhin Klassifikationen basierend auf den relativen Werten durch. Hierbei konzentrieren wir uns auf drei häufig genutzte Klassifikatoren und nutzen die Funktionen aus der Bibliothek scikit-learn von Pedregosa et al. \cite{PE11}. Die Ergebnisse werden anhand der folgenden drei Klassifikatoren ausgewertet und verglichen:

\begin{itemize}
\item Logistische Regression: Für zwei Klassen wird die \glqq One-Versus-Rest\grqq{} (ovr) Multi-Class-Strategie verwendet, während die Strategie für drei Klassen automatisch ausgewählt wird. Das Modell nutzt den 'lbfgs'-Solver und führt bis zu 1000 Iterationen durch, um das Modell zu trainieren.
\item K-Nearest Neighbour mit K=3.
\item Support Vector Machine.
\end{itemize}

Zudem untersuchen wir die Ergebnisse für drei Klassen (müde/fraglich/wach) sowie für zwei Klassen (müde/wach). Wir zogen auch die Entwicklung eines Scores in Erwägung, haben diese Idee jedoch aufgrund der begrenzten Datenmenge verworfen, da uns lediglich Daten für die Zustände \glqq müde\grqq{}, \glqq fraglich\grqq{} und \glqq wach\grqq{} zur Verfügung standen, jedoch keine weiteren Zwischenwerte.

Die Details zu den Klassifikationsergebnissen werden in der Evaluation in Kapitel \ref{subsec:classificatorcomparison} genauer analysiert. Für unser Projekt hat sich jedoch der K-Nearest Neighbour als die beste Option erwiesen. Um das System weiter zu verbessern, wäre eine größere Datenmenge von entscheidender Bedeutung. Zusätzlich könnten auch weitere Klassifikatoren in Betracht gezogen werden.

\subsection{Evaluationsmetriken zur Beurteilung der Klassifikatoren}
\label{sec:classificationmetrics}

Für die Evaluierung unserer Klassifikatoren haben wir verschiedene Metriken angewendet. Dabei haben wir die Funktionen aus der scikit-learn-Bibliothek von Pedregosa et al. \cite{PE11} genutzt. Konkret haben wir den Precision Score, Recall Score, F1 Score, Accuracy Score und die Fehlermatrix (auch als Confusion Matrix bekannt) verwendet.

Der Precision Score gibt an, wie viele der vorhergesagten positiven Instanzen tatsächlich positiv sind. Sein Wert liegt zwischen 0 und 1, wobei 1 für eine perfekte Präzision steht.

Der Recall Score zeigt, wie viele der tatsächlich positiven Instanzen korrekt als positiv vorhergesagt wurden. Auch hier reicht der Wertebereich von 0 bis 1, wobei 1 für einen perfekten Recall steht.

Der F1 Score ist das harmonische Mittel zwischen Precision und Recall und bietet ein ausgewogenes Maß für die Modellleistung. Auch dieser Wert liegt zwischen 0 und 1, wobei 1 eine perfekte Balance zwischen Precision und Recall repräsentiert.

Der Accuracy Score gibt das Verhältnis der korrekt klassifizierten Instanzen zur Gesamtanzahl der Instanzen im Testdatensatz an, wobei höhere Werte auf eine bessere Klassifikationsgenauigkeit hinweisen.

Die Fehlermatrix, auch als Confusion Matrix bezeichnet, ist eine tabellarische Darstellung der Anzahl der wahren positiven, wahren negativen, falsch positiven und falsch negativen Vorhersagen eines Klassifikationsmodells. Sie bietet eine aufgeschlüsselte Übersicht über die Verteilung der Ergebnisse.

\subsection{App-Entwicklung}
\label{ssec:appEntwicklung}
	%Beschreibung des Projekts
	%Ziele und Zweck der App
	Unser Projekt hat zum Ziel, eine Anwendung zu entwickeln, die die Müdigkeitserkennung durchführt. Diese App sollte auf verschiedenen Plattformen und Geräten zugänglich sein und es den Benutzern ermöglichen, ihre Müdigkeit in Echtzeit zu überwachen. Die App sollte einfach zu bedienen, effizient und zuverlässig sein. 
	
	\subsubsection{Technologieauswahl}
	\label{sssec:technologie}
		%Wahl der Programmiersprache (Python)
		%Entscheidung für das Kivy-Framework
		%Begründung für die Wahl dieser Technologien
		
		Zu Beginn der Technologieauswahl stand für uns bereits fest, dass Python das Mittel der Wahl für unsere Idee und Umsetzung aller benötigten Algorithmen ist. Wir begaben uns von Anfang an auf die Suche nach geeigneten Symbiosen, die uns die Entwicklung einer App mit Hilfe von Python ermöglichen.

		Als folge dessen haben wir uns bei der Auswahl der weiteren Technologien für unser Projekt intensiv mit den verschiedenen Optionen auseinandergesetzt. Eine entscheidende Überlegung war die Notwendigkeit der Offline-Funktionalität, da unsere App in Umgebungen zum Einsatz kommen sollte, in denen eine dauerhafte Internetverbindung nicht immer gewährleistet ist. Aus diesem Grund haben wir uns bewusst gegen die Realisierung einer API mit React und Flask entschieden. Hinter der Entscheidung stand ein intensives Erproben und Evaluieren der Möglichkeiten mit React und Flask. Schlussendlich wurde jedoch deutlich, dass eine andere Lösung gefunden werden muss.

		Nach genauerer Recherche, Erprobung und Evaluation fiel die Wahl auf Kivy. Die Entscheidung für das Kivy-Framework zur Frontend-Entwicklung war eine nahezu zwingende Konsequenz auf Grund von vielen Vorteilen. Kivy-Apps können auf verschiedenen Systemen zum Einsatz kommen und bieten somit eine breite Einsatzmöglichkeit. Zusätzlich synergiert Kivy mit Python und wurde einst für diese Programmiersprache entwickelt. Dies legte final den Grundstein für die Verbindung unseres Frontends mit unserer Logik, die ausschließlich auf Python basiert. Die Wahl von Kivy ermöglichte es uns, unser Projekt in den verschiedenen Fassetten starten und entwickeln zu können.
		
	\subsubsection{Entwicklungsprozess}
	\label{sssec:entwicklung}
		%Projektplanung und -management
		%Entwurf des Benutzeroberflächen-Designs
		%Implementierung und Code-Struktur
		
		Zu Beginn des Entwicklungsprozess war zwar entschieden, welche Technologien wir verwenden wollen, jedoch blieb die Frage der Implementierung noch aus. Es bildeten sich drei Teilbereiche, welche diese Frage beantworten konnten:
		\begin{itemize}
			\item Entwicklung des App-Grundgerüsts
			\item Entwicklung des Layouts der App
			\item Entwicklung der Integration von Logik und Layout
		\end{itemize}
		
		\noindent Ersteres definierte notwendige Schnittstellen und legte den Rahmen fest, in dem wir agierten. Es wurde eine Hauptdatei erstellt (mainmediapipe.py), welche die Logik des Backend aus einer externen Datei und ebenso das Layout beinhaltete und kombinierte. Zweiteres hingegen setzte alle notwendigen Entscheidungen bezüglich des Designs in einer Datei (mainmediapipe.kv) fest. Hierfür verwendeten wird das von Kivy mitgelieferte Dateiformat '.kv'. Letzteres beanspruchte am meisten Zeit, es mussten Verlinkungen zwischen der Logik und dem Layout vorgenommen werden. Es wurden Mechanismen für die visuelle und akustische Alarmierung entwickelt. Alles Schnittstellen wurden in Betrieb genommen und mussten untereinander kommunizieren können. Wir haben mittels der uns zur Verfügung stehenden Möglichkeiten all diese Hürden bezwingen können und als Produkt des Entwicklungsprozesses ein funktionierendes Gesamtpaket entstehen lassen.

		Zu diesem Zeitpunkt war die App rein desktopbasiert. Im Abschnitt \ref{ssec:lessonslearned} werden wir näher darauf eingehen, warum wir uns gegen die Fortsetzung der Bereitstellung bis zur Erstellung einer APK-Datei entschieden haben. Dabei werden wir auf die Herausforderungen und Lernprozesse eingehen, die diese Entscheidung beeinflusst haben.
		
	\subsubsection{Deployment-Strategie}
	\label{sssec:deployment}
		%Zielplattformen und -geräte
		%Deployment-Tools und -Methoden
		%Herausforderungen bei der Bereitstellung
		
		Das große Ziel war, eine funktionierende Software zu haben, die unseren Projektanforderungen gerecht wird. Darüber hinaus hatten wir noch einen weiteren selbstgewählten Anspruch, wir wollten eine App entwickeln, die sich über eine maximale Verfügbarkeit auszeichnet. Das heißt konkret, das Ziel war sich sukzessive von Softwarecode, zu Desktop-App bis hin zur APK-Datei zu verbessern. Dies war ein Vorhaben, welches über die Anforderungen des Modul hinaus ging. Der Start der Deployment-Strategie stellte die Betrachtung folgender Faktoren dar:
		
		\begin{itemize}
			\item Zielplattformen und -geräte
			\item Deployment-Tools und -Methoden
			\item Herausforderungen bei der Bereitstellung
		\end{itemize}
		
		\noindent Zielplattformen und -geräte sollten zunächst Windows und Linux abbilden und in zweiter Instanz Android. Deployment-Tools und -Methoden waren und durch unsere Technologieauswahl bereits bindend vorgegeben. Herausforderungen bei der Bereitstellung ergaben sich dann zu häuft, denn die Möglichkeit, eine App möglichst unkompliziert bereit zu stellen, enttarnte sich als starke Fehleinschätzung. Wir konnten relativ unkompliziert unseren Code als Desktop-App anbieten, der auf Windows und Linux funktionierte, jedoch war der weg zur APK-Datei wesentlich widerstandsfähiger. Es folgten diverse Versuche, einen Build-Prozess anzustoßen, welcher unseren Code zur benötigten Datei umbauen sollte. Jedoch hatten wir einen entscheidenden Fehler in der Technologieauswahl begangen, wir legten uns von Anfang an auf Python fest und limitierten uns damit ausschlaggebend in der Wahl der Build-Möglichkeiten. Für eine nähere Betrachtung dieses Fehlers verweisen wir im Abschnitt \ref{ssec:lessonslearned}.
